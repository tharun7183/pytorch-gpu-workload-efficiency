{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "gpuType": "T4"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# PyTorch GPU Workload Efficiency Benchmarks (Colab)\n", "\n", "This notebook generates reproducible GPU performance benchmarks + torch.profiler traces and creates batch-size sweep plots.\n", "\n", "**Important:** Run cells in order. After the install cell, Colab will restart automatically.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!nvidia-smi\n", "import torch\n", "print(\"CUDA available:\", torch.cuda.is_available())\n", "if torch.cuda.is_available():\n", "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Install pinned dependencies (Colab-safe)\n", "\n", "This avoids conflicts with Colab's preinstalled packages (e.g., pandas 2.2.2). The runtime will restart at the end of this cell.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip -q uninstall -y pandas tensorboard torch torchvision torchaudio 2>/dev/null\n", "\n", "!pip -q install pandas==2.2.2\n", "!pip -q install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n", "!pip -q install tensorboard==2.19.0 matplotlib==3.8.4\n", "\n", "import os\n", "os.kill(os.getpid(), 9)  # restart runtime\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Verify environment (run after restart)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch, pandas as pd, tensorboard\n", "print(\"torch:\", torch.__version__)\n", "print(\"cuda:\", torch.cuda.is_available())\n", "if torch.cuda.is_available():\n", "    print(\"gpu:\", torch.cuda.get_device_name(0))\n", "print(\"pandas:\", pd.__version__)\n", "print(\"tensorboard:\", tensorboard.__version__)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Write repo files\n", "\n", "Creates a mini repo under `/content/pytorch-gpu-workload-efficiency` with:\n", "- CLI benchmark runner\n", "- torch.profiler trace support\n", "- batch-size sweep\n", "- plotting scripts\n", "\n", "**Note:** `src/` is a proper Python package and is run via `python -m ...` to avoid import errors.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "import os, textwrap, pathlib\n", "\n", "ROOT = \"/content/pytorch-gpu-workload-efficiency\"\n", "os.makedirs(ROOT, exist_ok=True)\n", "\n", "def write(rel_path, content):\n", "    p = pathlib.Path(ROOT) / rel_path\n", "    p.parent.mkdir(parents=True, exist_ok=True)\n", "    p.write_text(textwrap.dedent(content))\n", "    return str(p)\n", "\n", "write(\".gitignore\", \"\"\"\n", ".venv/\n", "__pycache__/\n", "*.pyc\n", ".ipynb_checkpoints/\n", "outputs/\n", "assets/\n", "\"\"\")\n", "\n", "write(\"requirements.txt\", \"\"\"\n", "torch==2.5.1\n", "torchvision==0.20.1\n", "torchaudio==2.5.1\n", "pandas==2.2.2\n", "matplotlib==3.8.4\n", "tensorboard==2.19.0\n", "\"\"\")\n", "\n", "# Make src a package\n", "write(\"src/__init__.py\", \"\"\"\"\"\")\n", "\n", "write(\"src/metrics.py\", \"\"\"\n", "import torch\n", "import statistics\n", "\n", "def synchronize_if_cuda():\n", "    if torch.cuda.is_available():\n", "        torch.cuda.synchronize()\n", "\n", "def latency_stats(latencies_ms):\n", "    latencies_ms = list(latencies_ms)\n", "    latencies_ms.sort()\n", "\n", "    def pct(p):\n", "        if not latencies_ms:\n", "            return None\n", "        k = int(round((p/100) * (len(latencies_ms)-1)))\n", "        return latencies_ms[k]\n", "\n", "    return {\n", "        \"p50_ms\": pct(50),\n", "        \"p95_ms\": pct(95),\n", "        \"mean_ms\": statistics.mean(latencies_ms) if latencies_ms else None\n", "    }\n", "\"\"\")\n", "\n", "write(\"src/optimizations.py\", \"\"\"\n", "import torch\n", "from contextlib import nullcontext\n", "\n", "def apply_channels_last(model):\n", "    return model.to(memory_format=torch.channels_last)\n", "\n", "def maybe_compile(model, enabled: bool):\n", "    # torch.compile is available in PyTorch 2.x. If it fails on a given GPU, fall back gracefully.\n", "    if enabled and hasattr(torch, \"compile\"):\n", "        try:\n", "            return torch.compile(model)\n", "        except Exception:\n", "            return model\n", "    return model\n", "\n", "def autocast_ctx(enabled: bool):\n", "    if torch.cuda.is_available():\n", "        return torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=enabled)\n", "    return nullcontext()\n", "\"\"\")\n", "\n", "write(\"src/profilers.py\", \"\"\"\n", "import os\n", "import torch\n", "\n", "def maybe_profile(enabled: bool, profile_dir: str):\n", "    if not enabled:\n", "        return None\n", "\n", "    os.makedirs(profile_dir, exist_ok=True)\n", "\n", "    activities = [torch.profiler.ProfilerActivity.CPU]\n", "    if torch.cuda.is_available():\n", "        activities.append(torch.profiler.ProfilerActivity.CUDA)\n", "\n", "    prof = torch.profiler.profile(\n", "        activities=activities,\n", "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n", "        on_trace_ready=torch.profiler.tensorboard_trace_handler(profile_dir),\n", "        record_shapes=True,\n", "        profile_memory=True,\n", "        with_stack=False,\n", "    )\n", "    return prof\n", "\"\"\")\n", "\n", "write(\"src/benchmark_infer.py\", r\"\"\"\n", "import argparse, os, json, csv\n", "import torch\n", "import torchvision.models as models\n", "\n", "from .metrics import synchronize_if_cuda, latency_stats\n", "from .optimizations import apply_channels_last, maybe_compile, autocast_ctx\n", "from .profilers import maybe_profile\n", "\n", "def get_model(name: str):\n", "    name = name.lower()\n", "    if name == \"resnet50\":\n", "        return models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n", "    if name == \"efficientnet_b0\":\n", "        return models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n", "    raise ValueError(f\"Unsupported model: {name}\")\n", "\n", "def main():\n", "    ap = argparse.ArgumentParser()\n", "    ap.add_argument(\"--model\", default=\"resnet50\")\n", "    ap.add_argument(\"--batch-size\", type=int, default=32)\n", "    ap.add_argument(\"--steps\", type=int, default=120)\n", "    ap.add_argument(\"--warmup\", type=int, default=20)\n", "\n", "    ap.add_argument(\"--amp\", action=\"store_true\")\n", "    ap.add_argument(\"--channels-last\", action=\"store_true\")\n", "    ap.add_argument(\"--compile\", action=\"store_true\")\n", "\n", "    ap.add_argument(\"--profile\", action=\"store_true\")\n", "    ap.add_argument(\"--profile-dir\", default=\"outputs/profiler_traces\")\n", "\n", "    ap.add_argument(\"--out-csv\", default=\"outputs/results.csv\")\n", "    ap.add_argument(\"--out-json\", default=\"outputs/result.json\")\n", "    args = ap.parse_args()\n", "\n", "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "\n", "    model = get_model(args.model).eval().to(device)\n", "    if args.channels_last:\n", "        model = apply_channels_last(model)\n", "    model = maybe_compile(model, args.compile)\n", "\n", "    x = torch.randn(args.batch_size, 3, 224, 224, device=device)\n", "    if args.channels_last:\n", "        x = x.to(memory_format=torch.channels_last)\n", "\n", "    # Warmup\n", "    with torch.no_grad():\n", "        for _ in range(args.warmup):\n", "            with autocast_ctx(args.amp):\n", "                _ = model(x)\n", "        synchronize_if_cuda()\n", "\n", "    prof = maybe_profile(args.profile, args.profile_dir)\n", "    latencies = []\n", "    total_images = 0\n", "\n", "    with torch.no_grad():\n", "        if prof:\n", "            prof.__enter__()\n", "\n", "        for _ in range(args.steps):\n", "            if device == \"cuda\":\n", "                start = torch.cuda.Event(enable_timing=True)\n", "                end = torch.cuda.Event(enable_timing=True)\n", "                start.record()\n", "\n", "                with autocast_ctx(args.amp):\n", "                    _ = model(x)\n", "\n", "                end.record()\n", "                torch.cuda.synchronize()\n", "                ms = start.elapsed_time(end)\n", "            else:\n", "                import time\n", "                t0 = time.perf_counter()\n", "                with autocast_ctx(args.amp):\n", "                    _ = model(x)\n", "                t1 = time.perf_counter()\n", "                ms = (t1 - t0) * 1000.0\n", "\n", "            latencies.append(ms)\n", "            total_images += args.batch_size\n", "\n", "            if prof:\n", "                prof.step()\n", "\n", "        if prof:\n", "            prof.__exit__(None, None, None)\n", "\n", "    stats = latency_stats(latencies)\n", "    total_time_s = sum(latencies) / 1000.0\n", "    throughput = total_images / total_time_s if total_time_s > 0 else None\n", "\n", "    result = {\n", "        \"task\": \"inference\",\n", "        \"model\": args.model,\n", "        \"device\": device,\n", "        \"batch_size\": args.batch_size,\n", "        \"steps\": args.steps,\n", "        \"amp\": args.amp,\n", "        \"channels_last\": args.channels_last,\n", "        \"compile\": args.compile,\n", "        \"throughput_img_s\": throughput,\n", "        **stats,\n", "    }\n", "\n", "    os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)\n", "    write_header = not os.path.exists(args.out_csv)\n", "    with open(args.out_csv, \"a\", newline=\"\") as f:\n", "        w = csv.DictWriter(f, fieldnames=list(result.keys()))\n", "        if write_header:\n", "            w.writeheader()\n", "        w.writerow(result)\n", "\n", "    os.makedirs(os.path.dirname(args.out_json), exist_ok=True)\n", "    with open(args.out_json, \"w\") as f:\n", "        json.dump(result, f, indent=2)\n", "\n", "    print(json.dumps(result, indent=2))\n", "\n", "if __name__ == \"__main__\":\n", "    main()\n", "\"\"\")\n", "\n", "write(\"src/benchmark_train_micro.py\", r\"\"\"\n", "import argparse, os, json, csv\n", "import torch\n", "import torchvision.models as models\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "\n", "from .metrics import synchronize_if_cuda, latency_stats\n", "from .optimizations import apply_channels_last, maybe_compile, autocast_ctx\n", "from .profilers import maybe_profile\n", "\n", "def get_model(name: str, num_classes=1000):\n", "    name = name.lower()\n", "    if name == \"resnet50\":\n", "        return models.resnet50(weights=None, num_classes=num_classes)\n", "    if name == \"efficientnet_b0\":\n", "        return models.efficientnet_b0(weights=None, num_classes=num_classes)\n", "    raise ValueError(f\"Unsupported model: {name}\")\n", "\n", "def main():\n", "    ap = argparse.ArgumentParser()\n", "    ap.add_argument(\"--model\", default=\"resnet50\")\n", "    ap.add_argument(\"--batch-size\", type=int, default=32)\n", "    ap.add_argument(\"--steps\", type=int, default=80)\n", "    ap.add_argument(\"--warmup\", type=int, default=10)\n", "\n", "    ap.add_argument(\"--amp\", action=\"store_true\")\n", "    ap.add_argument(\"--channels-last\", action=\"store_true\")\n", "    ap.add_argument(\"--compile\", action=\"store_true\")\n", "\n", "    ap.add_argument(\"--profile\", action=\"store_true\")\n", "    ap.add_argument(\"--profile-dir\", default=\"outputs/profiler_traces\")\n", "\n", "    ap.add_argument(\"--out-csv\", default=\"outputs/results.csv\")\n", "    ap.add_argument(\"--out-json\", default=\"outputs/train_result.json\")\n", "    args = ap.parse_args()\n", "\n", "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "    torch.manual_seed(0)\n", "\n", "    model = get_model(args.model).train().to(device)\n", "    if args.channels_last:\n", "        model = apply_channels_last(model)\n", "    model = maybe_compile(model, args.compile)\n", "\n", "    criterion = nn.CrossEntropyLoss()\n", "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n", "\n", "    x = torch.randn(args.batch_size, 3, 224, 224, device=device)\n", "    y = torch.randint(0, 1000, (args.batch_size,), device=device)\n", "    if args.channels_last:\n", "        x = x.to(memory_format=torch.channels_last)\n", "\n", "    scaler = torch.cuda.amp.GradScaler(enabled=(args.amp and device == \"cuda\"))\n", "\n", "    # Warmup\n", "    for _ in range(args.warmup):\n", "        optimizer.zero_grad(set_to_none=True)\n", "        with autocast_ctx(args.amp):\n", "            out = model(x)\n", "            loss = criterion(out, y)\n", "        scaler.scale(loss).backward()\n", "        scaler.step(optimizer)\n", "        scaler.update()\n", "    synchronize_if_cuda()\n", "\n", "    prof = maybe_profile(args.profile, args.profile_dir)\n", "    it_ms = []\n", "    total_images = 0\n", "\n", "    if prof:\n", "        prof.__enter__()\n", "\n", "    for _ in range(args.steps):\n", "        if device == \"cuda\":\n", "            start = torch.cuda.Event(enable_timing=True)\n", "            end = torch.cuda.Event(enable_timing=True)\n", "            start.record()\n", "\n", "        optimizer.zero_grad(set_to_none=True)\n", "        with autocast_ctx(args.amp):\n", "            out = model(x)\n", "            loss = criterion(out, y)\n", "        scaler.scale(loss).backward()\n", "        scaler.step(optimizer)\n", "        scaler.update()\n", "\n", "        if device == \"cuda\":\n", "            end.record()\n", "            torch.cuda.synchronize()\n", "            ms = start.elapsed_time(end)\n", "        else:\n", "            import time\n", "            t0 = time.perf_counter()\n", "            optimizer.step()\n", "            t1 = time.perf_counter()\n", "            ms = (t1 - t0) * 1000.0\n", "\n", "        it_ms.append(ms)\n", "        total_images += args.batch_size\n", "\n", "        if prof:\n", "            prof.step()\n", "\n", "    if prof:\n", "        prof.__exit__(None, None, None)\n", "\n", "    stats = latency_stats(it_ms)\n", "    total_time_s = sum(it_ms) / 1000.0\n", "    throughput = total_images / total_time_s if total_time_s > 0 else None\n", "\n", "    result = {\n", "        \"task\": \"train_micro\",\n", "        \"model\": args.model,\n", "        \"device\": device,\n", "        \"batch_size\": args.batch_size,\n", "        \"steps\": args.steps,\n", "        \"amp\": args.amp,\n", "        \"channels_last\": args.channels_last,\n", "        \"compile\": args.compile,\n", "        \"throughput_img_s\": throughput,\n", "        **stats,\n", "    }\n", "\n", "    os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)\n", "    write_header = not os.path.exists(args.out_csv)\n", "    with open(args.out_csv, \"a\", newline=\"\") as f:\n", "        w = csv.DictWriter(f, fieldnames=list(result.keys()))\n", "        if write_header:\n", "            w.writeheader()\n", "        w.writerow(result)\n", "\n", "    os.makedirs(os.path.dirname(args.out_json), exist_ok=True)\n", "    with open(args.out_json, \"w\") as f:\n", "        json.dump(result, f, indent=2)\n", "\n", "    print(json.dumps(result, indent=2))\n", "\n", "if __name__ == \"__main__\":\n", "    main()\n", "\"\"\")\n", "\n", "write(\"scripts/sweep_batch.py\", r\"\"\"\n", "import os, subprocess\n", "import pandas as pd\n", "\n", "BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]\n", "MODEL = \"resnet50\"\n", "\n", "OUT = \"outputs/sweep_results.csv\"\n", "os.makedirs(\"outputs\", exist_ok=True)\n", "\n", "if os.path.exists(OUT):\n", "    os.remove(OUT)\n", "\n", "def run_one(bs, amp=False, channels_last=False, compile_=False, tag=\"\"):\n", "    cmd = [\n", "        \"python\", \"-m\", \"src.benchmark_infer\",\n", "        \"--model\", MODEL,\n", "        \"--batch-size\", str(bs),\n", "        \"--steps\", \"120\",\n", "        \"--warmup\", \"20\",\n", "        \"--out-csv\", OUT,\n", "        \"--out-json\", f\"outputs/sweep_{tag}_bs{bs}.json\",\n", "    ]\n", "    if amp:\n", "        cmd.append(\"--amp\")\n", "    if channels_last:\n", "        cmd.append(\"--channels-last\")\n", "    if compile_:\n", "        cmd.append(\"--compile\")\n", "\n", "    print(\"RUN:\", \" \".join(cmd))\n", "    subprocess.run(cmd, check=True)\n", "\n", "# baseline sweep\n", "for bs in BATCH_SIZES:\n", "    run_one(bs, amp=False, channels_last=False, compile_=False, tag=\"baseline\")\n", "\n", "# optimized sweep\n", "for bs in BATCH_SIZES:\n", "    run_one(bs, amp=True, channels_last=True, compile_=True, tag=\"optimized\")\n", "\n", "df = pd.read_csv(OUT).sort_values([\"amp\", \"channels_last\", \"compile\", \"batch_size\"])\n", "print(df)\n", "print(\"\\nSaved:\", OUT)\n", "\"\"\")\n", "\n", "write(\"scripts/plot_sweep.py\", r\"\"\"\n", "import os\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "IN_CSV = \"outputs/sweep_results.csv\"\n", "os.makedirs(\"assets\", exist_ok=True)\n", "\n", "df = pd.read_csv(IN_CSV)\n", "\n", "# Throughput plot\n", "plt.figure()\n", "for (amp, ch, comp), g in df.groupby([\"amp\", \"channels_last\", \"compile\"]):\n", "    g = g.sort_values(\"batch_size\")\n", "    label = f\"amp={amp}, cl={ch}, compile={comp}\"\n", "    plt.plot(g[\"batch_size\"], g[\"throughput_img_s\"], marker=\"o\", label=label)\n", "\n", "plt.xscale(\"log\", base=2)\n", "plt.xlabel(\"Batch size\")\n", "plt.ylabel(\"Throughput (img/s)\")\n", "plt.title(\"ResNet-50 Inference Throughput vs Batch Size\")\n", "plt.grid(True)\n", "plt.legend()\n", "plt.savefig(\"assets/batch_sweep_throughput.png\", dpi=200, bbox_inches=\"tight\")\n", "plt.close()\n", "\n", "# p50 latency plot\n", "plt.figure()\n", "for (amp, ch, comp), g in df.groupby([\"amp\", \"channels_last\", \"compile\"]):\n", "    g = g.sort_values(\"batch_size\")\n", "    label = f\"amp={amp}, cl={ch}, compile={comp}\"\n", "    plt.plot(g[\"batch_size\"], g[\"p50_ms\"], marker=\"o\", label=label)\n", "\n", "plt.xscale(\"log\", base=2)\n", "plt.xlabel(\"Batch size\")\n", "plt.ylabel(\"p50 latency (ms)\")\n", "plt.title(\"ResNet-50 Inference p50 Latency vs Batch Size\")\n", "plt.grid(True)\n", "plt.legend()\n", "plt.savefig(\"assets/batch_sweep_latency.png\", dpi=200, bbox_inches=\"tight\")\n", "plt.close()\n", "\n", "print(\"Saved plots:\")\n", "print(\" - assets/batch_sweep_throughput.png\")\n", "print(\" - assets/batch_sweep_latency.png\")\n", "\"\"\")\n", "\n", "write(\"README.md\", \"\"\"\n", "# PyTorch GPU Workload Efficiency Benchmark Suite\n", "\n", "Reproducible benchmarking + profiling toolkit for PyTorch training and inference on GPU.\n", "\n", "## What it measures\n", "- **Throughput:** images/sec\n", "- **Latency:** p50 / p95 (ms)\n", "\n", "## Quickstart (Colab)\n", "Run the notebook `notebooks/GWE_Benchmarks_Colab.ipynb`.\n", "\n", "## Profiling\n", "Profiler traces are written to `outputs/profiler_traces/` and can be viewed in TensorBoard.\n", "\n", "## Batch sweep plots\n", "Plots are saved in `assets/`.\n", "\"\"\")\n", "\n", "print(\"Wrote repo to:\", ROOT)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Run baseline + optimized benchmarks\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%cd /content/pytorch-gpu-workload-efficiency\n", "!rm -f outputs/results.csv\n", "\n", "# Inference baseline\n", "!python -m src.benchmark_infer --model resnet50 --batch-size 32 --steps 150 --warmup 20 --out-csv outputs/results.csv --out-json outputs/infer_baseline.json\n", "\n", "# Inference optimized (AMP + channels-last + compile)\n", "!python -m src.benchmark_infer --model resnet50 --batch-size 32 --steps 150 --warmup 20 --amp --channels-last --compile --out-csv outputs/results.csv --out-json outputs/infer_optimized.json\n", "\n", "# Micro-train baseline\n", "!python -m src.benchmark_train_micro --model resnet50 --batch-size 32 --steps 80 --warmup 10 --out-csv outputs/results.csv --out-json outputs/train_baseline.json\n", "\n", "# Micro-train optimized (AMP + channels-last)\n", "!python -m src.benchmark_train_micro --model resnet50 --batch-size 32 --steps 80 --warmup 10 --amp --channels-last --out-csv outputs/results.csv --out-json outputs/train_optimized.json\n", "\n", "!python - <<'PY'\n", "import pandas as pd\n", "df = pd.read_csv(\"outputs/results.csv\")\n", "display(df.sort_values([\"task\",\"amp\",\"channels_last\",\"compile\"]))\n", "PY\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Generate a profiler trace + open TensorBoard\n", "\n", "1. Run the trace cell\n", "2. Run TensorBoard cell\n", "3. Screenshot the **Profile \u2192 Trace viewer** page and upload to `assets/profiler_trace.png`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%cd /content/pytorch-gpu-workload-efficiency\n", "!rm -rf outputs/profiler_traces\n", "!mkdir -p outputs/profiler_traces\n", "\n", "!python -m src.benchmark_infer --model resnet50 --batch-size 32 --steps 30 --warmup 10 --amp --profile --profile-dir outputs/profiler_traces --out-csv outputs/results.csv --out-json outputs/profile_run.json\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%load_ext tensorboard\n", "%tensorboard --logdir /content/pytorch-gpu-workload-efficiency/outputs/profiler_traces\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Batch-size sweep + plots\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%cd /content/pytorch-gpu-workload-efficiency\n", "!python scripts/sweep_batch.py\n", "!python scripts/plot_sweep.py\n", "!ls -lah assets\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Package for GitHub upload\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%cd /content\n", "!zip -r pytorch-gpu-workload-efficiency.zip pytorch-gpu-workload-efficiency\n", "print(\"Zip created: /content/pytorch-gpu-workload-efficiency.zip\")\n"]}]}
